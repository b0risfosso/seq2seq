{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99633c69-7c87-458b-9cb5-b252758fb874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/b/ml/seq2seq/venv/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b03d45f-5719-4d31-9190-8191b427a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import tqdm\n",
    "import evaluate\n",
    "import matplotlib.pyplot as pyplot\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92f62b9-799c-463c-8f39-f5144a032a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72e64277-7101-490f-9a74-02a5fd93efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset found at bentrevett/multi30k\n",
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9efcdf9d-d777-4aaa-8399-1cc73950964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "626ecf92-0a74-47f1-ac58-c87e25524fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebccd5a8-55ff-44d7-816a-0df256e3b61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76beeeb8aa2c4dc281002128f4b69f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd435a10df546a286985e48f4685671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16b715db1c9485e8b783e5ca1f54805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 1000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0852ff91-c4d4-4bf0-b15a-4eb5fc9c8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, token_to_index, unk_token=\"<unk>\"):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {idx:token for token in token_to_index.items()}\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = token_to_index[unk_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_index.get(token, self.unk_index)\n",
    "        \n",
    "    def token_to_idx(self, token):\n",
    "        return self.__getitem__(token)\n",
    "\n",
    "    def idx_to_token(self, idx):\n",
    "        return self.index_to_token.get(idx, self.unk_token)\n",
    "\n",
    "    def set_default_index(self, index):\n",
    "        if index==self.unk_index:\n",
    "            return\n",
    "\n",
    "        current_token = self.index_to_token.get(index, None)\n",
    "\n",
    "        if self.unk_token in self.token_to_index and current_token:\n",
    "            self.token_to_index[self.unk_token], self.token_to_index[current_token] = index, self.unk_index\n",
    "\n",
    "        if self.unk_index in self.index_to_token and index in self.index_to_token:\n",
    "            self.index_to_token[self.unk_index], self.index_to_token[index] = self.index_to_token[index], self.index_to_token[self.unk_index]\n",
    "\n",
    "        self.unk_index = index\n",
    "    \n",
    "    def get_itos(self):\n",
    "        return self.index_to_token\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.token_to_index\n",
    "\n",
    "    def lookup_indices(self, tokens):\n",
    "        return [self.token_to_idx(token) for token in tokens]\n",
    "\n",
    "    def lookup_tokens(self, indices):\n",
    "        if torch.istensor(indices):\n",
    "            indices = indices.tolist()\n",
    "\n",
    "        return [self.idx_to_token(index) for index in indices]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf8895-bd19-4d02-82d8-c32a2218089e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
