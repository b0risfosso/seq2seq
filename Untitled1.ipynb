{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab9f8517-ccb5-4b16-8d6c-e003d65bf336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy\n",
    "import datasets\n",
    "#import torchtext\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5e75cbe-84ee-4cf3-934a-775515b4d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5442f5e5-77c1-4bab-ac83-5edb095b725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db825cc-0d7a-43f0-9d4e-2e91680b61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4247515-35c4-43be-94e0-7349c375a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e616ef0-6f4e-41ff-bc47-4a112b3c0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        de_tokens = [token.lower() for token in de_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a2511fc-7091-4dfc-bc14-75b30d27ca3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c125178dfa40f1af479b8e0933fac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d7841205994a379b7ece80d0827c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671909e6bc90455e808028d5564b491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 1000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"de_nlp\": de_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd018db2-79dd-40a7-a3a2-f0267de1f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [unk_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "en_voa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e6580-d5ce-4711-9769-6f0740ebc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab class and build_vocab_from_iterator method\n",
    "# build the build_vocab_from_iterator\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, token_to_index, unk_token=\"<unk>\"):\n",
    "        self.token_to_index = token_to_index\n",
    "        self.index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "        self.unk_token = unk_token\n",
    "        self.unk_index = token_to_index[unk_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_index.get(token, self.unk_index)\n",
    "\n",
    "    def token_to_idx(self, token):\n",
    "        return self.__getitem__(token)\n",
    "\n",
    "    def idx_to_token(self, idx):\n",
    "        return self.index_to_token.get(idx, self.unk_token)\n",
    "\n",
    "    def get_itos(self):\n",
    "        \"\"\"Returns the index-to-string mapping (i.e., list of tokens).\"\"\"\n",
    "        # Creating a list with tokens where the index corresponds to the token's position\n",
    "        max_index = max(self.index_to_token.keys())\n",
    "        itos = [self.index_to_token.get(i, self.unk_token) for i in range(max_index + 1)]\n",
    "        return itos\n",
    "\n",
    "    def get_stoi(self):\n",
    "        \"\"\"Returns the string-to-index mapping (i.e., dictionary of tokens and their indices).\"\"\"\n",
    "        return self.token_to_index\n",
    "\n",
    "    def set_default_index(self, index):\n",
    "        \"\"\"Sets the default index for unknown tokens.\"\"\"\n",
    "        self.unk_index = index\n",
    "\n",
    "    def lookup_indices(self, tokens):\n",
    "        \"\"\"Returns a list of indices for the given list of tokens.\"\"\"\n",
    "        return [self.token_to_idx(token) for token in tokens]\n",
    "\n",
    "    def lookup_tokens(self, indices):\n",
    "        \"\"\"Returns a list of tokens for the given list of indices.\"\"\"\n",
    "        if torch.is_tensor(indices):\n",
    "            indices = indices.tolist()\n",
    "        return [self.idx_to_token(index) for index in indices]\n",
    "\n",
    "\n",
    "def build_vocab_from_iterator(iterator, min_freq=1, specials=None):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from an iterator.\n",
    "\n",
    "    Args:\n",
    "    - iterator (iterable): An iterable yielding lists of tokens.\n",
    "    - min_freq (int): The minimum frequency a token must have to be included in the vocabulary.\n",
    "    - specials (list): A list of special tokens (e.g., ['<unk>', '<pad>', '<sos>', '<eos>']).\n",
    "\n",
    "    Returns:\n",
    "    - vocab (Vocab): A custom Vocab object containing the token-to-index mapping.\n",
    "    \"\"\"\n",
    "    # Initialize the counter for token frequencies\n",
    "    counter = Counter()\n",
    "\n",
    "    # Count frequencies of tokens in the iterator\n",
    "    for tokens in iterator:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # Start with special tokens if provided\n",
    "    token_to_index = {}\n",
    "    if specials:\n",
    "        for idx, token in enumerate(specials):\n",
    "            token_to_index[token] = idx\n",
    "    \n",
    "    # Add regular tokens to the vocabulary if they meet the min_freq\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq and token not in token_to_index:\n",
    "            token_to_index[token] = len(token_to_index)\n",
    "\n",
    "    # Set the <unk> token index if it's not already set\n",
    "    unk_token = specials[0] if specials else \"<unk>\"\n",
    "    if unk_token not in token_to_index:\n",
    "        token_to_index[unk_token] = len(token_to_index)\n",
    "\n",
    "    print(token_to_index)\n",
    "\n",
    "    return Vocab(token_to_index, unk_token=unk_token)\n",
    "\n",
    "# Example usage:\n",
    "tokens = [[\"i\", \"love\", \"pizza\"], [\"i\", \"hate\", \"music\", \"videos\"]]\n",
    "specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "vocab = build_vocab_from_iterator(tokens, min_freq=1, specials=specials)\n",
    "print(vocab.token_to_index)\n",
    "print(vocab.idx_to_token(6))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
